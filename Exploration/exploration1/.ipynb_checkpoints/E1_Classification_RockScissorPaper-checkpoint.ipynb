{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0b96e6",
   "metadata": {},
   "source": [
    "# 가위바위보 분류기  \n",
    "  \n",
    "  \n",
    "  \n",
    "- 직접 촬영한 가위, 바위, 보 사진을 촬영\n",
    "- 모델을 만들어서 학습을 한다.\n",
    "- 학습된 모델을 평가 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a562e7",
   "metadata": {},
   "source": [
    "## 1. Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fd8ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06d99b",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "- 가위바위보 사진 각각 300장을 준비한다.\n",
    "- 가위바위보 사진을 28 x 28 x 3으로 reshape한다\n",
    "- 각각의 데이터에 label을 붙임\n",
    "- nomlization을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952cc7f",
   "metadata": {},
   "source": [
    "### 2-1 Image resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e9f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    \n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)        \n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(f\"{len(images)}images resized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17317eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  images to be resized.\n",
      "300images resized.\n",
      "300  images to be resized.\n",
      "300images resized.\n",
      "300  images to be resized.\n",
      "300images resized.\n",
      "100  images to be resized.\n",
      "100images resized.\n",
      "100  images to be resized.\n",
      "100images resized.\n",
      "100  images to be resized.\n",
      "100images resized.\n"
     ]
    }
   ],
   "source": [
    "rock_image_dir_path = \"./train/rock\"\n",
    "scissor_image_dir_path = \"./train/scissor\"\n",
    "paper_image_dir_path = \"./train/paper\"\n",
    "\n",
    "resize_images(rock_image_dir_path)\n",
    "resize_images(scissor_image_dir_path)\n",
    "resize_images(paper_image_dir_path)\n",
    "\n",
    "rock_image_dir_path = \"./test/rock\"\n",
    "scissor_image_dir_path = \"./test/scissor\"\n",
    "paper_image_dir_path = \"./test/paper\"\n",
    "\n",
    "resize_images(rock_image_dir_path)\n",
    "resize_images(scissor_image_dir_path)\n",
    "resize_images(paper_image_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b925da",
   "metadata": {},
   "source": [
    "### 2-2 Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45ecec45",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-b3b37d8cf9fc>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-b3b37d8cf9fc>\"\u001b[1;36m, line \u001b[1;32m28\u001b[0m\n\u001b[1;33m    print(f\"데이터 X의 이미지 개수는\"{idx}\"입니다.\")\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def load_data(img_path, number_of_data=900):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx = 0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)        \n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "    \n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):        \n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        \n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1      \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)        \n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(f\"데이터 X의 이미지 개수는\"{idx}\"입니다.\")\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac70a374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 X의 이미지 개수는 900 입니다.\n",
      "데이터 X의 이미지 개수는 300 입니다.\n",
      "x_train shape: (900, 28, 28, 3)\n",
      "y_train shape: (900,)\n",
      "x_test shape: (900, 28, 28, 3)\n",
      "y_test shape: (900,)\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = \"./train/\"\n",
    "(x_train, y_train)=load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "image_dir_path = \"./test/\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_train/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(f\"x_train shape: {format(x_train.shape)}\")\n",
    "print(f\"y_train shape: {format(y_train.shape)}\")\n",
    "\n",
    "print(f\"x_test shape: {format(x_test.shape)}\")\n",
    "print(f\"y_test shape: {format(y_test.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d331cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])\n",
    "print(f'라벨: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01247fa7",
   "metadata": {},
   "source": [
    "## 3.Modeling\n",
    "- keras를 활용하여 간단한 CNN 기반 분류기 모델을 정의\n",
    "- 채널수는 각각 64,64,64개를 가짐\n",
    "- 최종적으로 분류 개수는 3으로 해야하지만 5로 할때 더 높은 성능을 나타냄\n",
    "- epoch수는 10으로 지정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "817464f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 80,133\n",
      "Trainable params: 80,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 12ms/step - loss: 3.8549 - accuracy: 0.4422\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.5592 - accuracy: 0.8011\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.3466 - accuracy: 0.8867\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.3018 - accuracy: 0.8967\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1543 - accuracy: 0.9578\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1187 - accuracy: 0.9644\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1497 - accuracy: 0.9433\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.1192 - accuracy: 0.9544\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0508 - accuracy: 0.9844\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0436 - accuracy: 0.9911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12811e7a7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channel_1=64\n",
    "n_channel_2=64\n",
    "n_channel_3=64\n",
    "n_dense=64\n",
    "n_train_epoch=10\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(n_channel_3, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa32357",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "- optimizer : adam \n",
    "- loss function : sparse_categorical_crossentropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2632ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 1s 12ms/step - loss: 0.8326 - accuracy: 0.8244\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0881 - accuracy: 0.9756\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0510 - accuracy: 0.9900\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0354 - accuracy: 0.9911\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0282 - accuracy: 0.9922\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0520 - accuracy: 0.9822\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0836 - accuracy: 0.9711\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0315 - accuracy: 0.9922\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0332 - accuracy: 0.9878\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 12ms/step - loss: 0.0248 - accuracy: 0.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12819033e48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train, y_train,epochs=n_train_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99c0a0",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "- 약 81%의 정확도를 나타낸다.\n",
    "- 내부 구조나 다양한 최적화 기법을 사용하면 더 정확해 질것으로 생각된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "101e5ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 0s - loss: 1.8518 - accuracy: 0.8189\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f745f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
